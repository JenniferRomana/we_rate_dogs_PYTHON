{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs - Wrangle and Analyze Data\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "<br><br>\n",
    "- [Import libaries](#import)\n",
    "<br><br>\n",
    "- [Gather data](#gather)\n",
    "    1. [File on Hand](#gather_1)\n",
    "    2. [File from Internet](#gather_2)\n",
    "    3. [API data](#gather_3)\n",
    "<br><br>\n",
    "- [Assess Data](#assess)\n",
    "    1. [File on Hand: df_twitter_archive](#assess_1)\n",
    "    2. [File from Internet: df_images](#assess_2)\n",
    "    3. [API data: df_tweets](#assess_3)\n",
    "    4. [Quality Issues](#assess_qual)\n",
    "    5. [Tidiness Issues](#assess_tidy)\n",
    "<br><br>\n",
    "- [Clean Data](#clean)\n",
    "    1. [Missing Data](#clean_miss)\n",
    "    2. [Tidiness Issues](#clean_tidy)\n",
    "    3. [Quality Issues](#clean_qual)\n",
    "<br><br>\n",
    "- [Save Cleaned Datasets](#store)\n",
    "<br><br>\n",
    "- [Analyze and Visualize](#analyze)\n",
    "<br><br>\n",
    "- [Conclusions](#conclusion)\n",
    "\n",
    "    \n",
    "    \n",
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "*WeRateDogs* is a Twitter account that gives humourous comments and ratings of people's dogs.  It has over 8 million followers and international media coverage.\n",
    "\n",
    "In order to find interesting insights about whether and how the Dog Rating Tweets relate to the popularity of *WeRateDogs* Twitter account, data must first be gathered, assessed and clean.  Three datasets are available for this project:\n",
    "   1. File on hand: WeRateDogs Twitter archive, which includes basic tweet data for over 2,000 tweets that have ratings\n",
    "   2. File from the internet: Image Prediction file that gives the top 3 predictions of a dog's breed based on a jpeg image associated with each tweet that has been run through a neural network.  This file is hosted on Udacity's server\n",
    "   3. Twitter API data: query Twitter's API in order to gather additional and valuable information.  Save the JSON data to text\n",
    "    \n",
    "The data is then assessed both visually and programatically to find any quality or tidiness issues.  Quality issues relate to the data content and Tidiness issues relate to the structure of the data.  Each issue is then in turn treated and cleaned.\n",
    "\n",
    "Finally the data will be explored, guided by questions about how the Dog Rating Tweets relate to the phenomenal success and popularity of *WeRateDogs* Twitter account.\n",
    "\n",
    "#### *Key Points*\n",
    "- We only want original tweets (no retweets) that have images\n",
    "- The fact that the rating numerators are greater than the denominators does not need to be cleaned. This unique rating system is a big part of the popularity of *WeRateDogs*.\n",
    "- Tweets do not extend beyond August 1, 2017\n",
    "\n",
    "\n",
    "\n",
    "<a id='import'></a>\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import tweepy\n",
    "import wptools\n",
    "\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather'></a>\n",
    "## Gather\n",
    "\n",
    "<a id='gather_1'></a>\n",
    "### *Gather 1. File on Hand*\n",
    "\n",
    "WeRateDogs Twitter archive, `twitter-archive-enhanced.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in file on hand twitter-archive-enhanced.csv\n",
    "df_twitter_archive = pd.read_csv('./data/twitter-archive-enhanced.csv')\n",
    "df_twitter_archive.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather_2'></a>\n",
    "### *Gather 2. File from internet*\n",
    "\n",
    "Image Prediction file `image_predictions.tsv`is hosted on Udacity's server at  https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and/or Create folder for data\n",
    "folder_name = 'data'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download image Tweet image prediction file \n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write content to local computer\n",
    "with open(os.path.join(folder_name,\n",
    "                      url.split('/')[-1]), mode='wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check encoding on downloaded file\n",
    "with open('{}/image-predictions.tsv'.format(folder_name)) as file:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read image prediction data into DataFrame df_images\n",
    "df_images = pd.read_csv('{}/image-predictions.tsv'.format(folder_name), sep='\\t')\n",
    "df_images.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather_3'></a>\n",
    "### *Gather 3. Twitter's API data*\n",
    "Query Twitter's API using Tweepy and save JSON in a text file `tweet_json.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hidden Twitter API keys, secrets and tokens\n",
    "import creds\n",
    "auth = tweepy.OAuthHandler(creds.consumer_key, creds.consumer_secret)\n",
    "auth.set_access_token(creds.access_token, creds.access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tweet_id list based on tweet_id in Twitter archive df_twitter_archive\n",
    "tweet_id_list = df_twitter_archive['tweet_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query Twitter's API for JSON data for each tweet_id in tweet_id_list\n",
    "tweet_data=[]\n",
    "tweet_error={}\n",
    "\n",
    "tweet_count=0\n",
    "error_count=0\n",
    "\n",
    "start=timer()\n",
    "\n",
    "for tweet_id in tweet_id_list:\n",
    "    try:\n",
    "        tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "        tweet_data.append(tweet._json)\n",
    "        tweet_count +=1\n",
    "        print(tweet.id)\n",
    "        \n",
    "    except Exception as e:\n",
    "        tweet_error[str(tweet_id)]=str(e)\n",
    "        error_count +=1\n",
    "        print(str(tweet_id) + \": \" + str(e))\n",
    "        \n",
    "end=timer()\n",
    "print('Elapsed time: {}'.format(end-start))\n",
    "print('Tweet count: {}'.format(tweet_count))\n",
    "print('Error count: {}'.format(error_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually get data for tweet_ids if error is other than 'No status found with that ID'\n",
    "for key in tweet_error.keys():\n",
    "    if not (\"'code': 144\" in tweet_error.get(key)):\n",
    "        tweet = api.get_status(key, tweet_mode='extended')\n",
    "        tweet_data.append(tweet._json)\n",
    "        print(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Tweet JSON data to tweet_json.txt\n",
    "with open('{}/tweet_json.txt'.format(folder_name), 'w') as outfile:\n",
    "    for json_tweet in tweet_data:\n",
    "        json.dump(json_tweet, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json_tweet.txt line by line to extract tweet_id, retweet_count and favourite_count\n",
    "json_tweet_data = []\n",
    "with open('{}/tweet_json.txt'.format(folder_name)) as f:\n",
    "    for line in f:\n",
    "        json_data = json.loads(line)\n",
    "        tweet_id = json_data['id']\n",
    "        retweet_count = json_data['retweet_count']\n",
    "        favourite_count = json_data['favorite_count']\n",
    "        json_tweet_data.append({'tweet_id': tweet_id,\n",
    "                                'retweet_count': retweet_count,\n",
    "                               'favourite_count': favourite_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.DataFrame(json_tweet_data, columns=['tweet_id','retweet_count', 'favourite_count'])\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "## Assess\n",
    "\n",
    "Assess the data first visually then programatically for Quality and Tidiness issues.\n",
    "\n",
    "***Quality*** issues relate to content.  The four main quality dimensions are:\n",
    "- *Completeness* : do we have all the records that we should and are the specific rows, columns or cells missing?\n",
    "- *Validitiy* : does the date conform to the defined schema, or set of rules?\n",
    "- *Accuracy* : data that may conform to the schema but is still incorrect\n",
    "- *Consistency* : is data presented consistently within and across tables\n",
    "\n",
    "***Tidiness*** issues relate to structure. The basic principals of tidy data are:\n",
    "- Each variable forms a column\n",
    "- Each observation forms a row\n",
    "- Each type of observational unit forms a table\n",
    "\n",
    "\n",
    "<a id='assess_1'></a>\n",
    "### *Assess 1. File on Hand: df_twitter_archive*\n",
    "\n",
    "*WeRateDogs* Twitter archive, which includes basic tweet data for over 2,000 tweets that have ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_twitter_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_twitter_archive.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Columns* \n",
    "- *tweet_id*:unique identifier for the Tweet                      \n",
    "- *in_reply_to_status_id*: Nullable. If the Tweet is a reply, this field will contain the original Tweet’s ID.       \n",
    "- *in_reply_to_user_id*: Nullable. If the Tweet is a reply, this field will contain the original Tweet’s author ID. This will not necessarily always be the user directly mentioned in the Tweet.        \n",
    "- *timestamp*: UTC time when this Tweet was created                   \n",
    "- *source*: Utility used to post the Tweet, as an HTML-formatted string                       \n",
    "- *text*: The actual UTF-8 text of the status update.                       \n",
    "- *retweeted_status_id*:           \n",
    "- *retweeted_status_user_id*:       \n",
    "- *retweeted_status_timestamp*:     \n",
    "- *expanded_urls*: Links to the media display page, expanded version                \n",
    "- *rating_numerator*             \n",
    "- *rating_denominator*            \n",
    "- *name*: name of dog, if name is present in text                        \n",
    "- *doggo*: ('doggo'/'None') based on description in text.                     \n",
    "- *floofer*: ('floofer'/'None') based on description in text.                      \n",
    "- *pupper*: ('pupper'/'None') based on description in text.                        \n",
    "- *puppo*: ('puppo'/'None') based on description in text.                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_twitter_archive.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_twitter_archive['tweet_id'].duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all missing 'expanded_urls' are due to retweets and replies\n",
    "df_twitter_archive[df_twitter_archive['expanded_urls'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_twitter_archive['expanded_urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_twitter_archive[df_twitter_archive['expanded_urls'].str.strip().str[:15]!='https://twitter']['expanded_urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_twitter_archive.sample(10)['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive['rating_denominator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_twitter_archive['rating_denominator']>10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_twitter_archive[df_twitter_archive['rating_denominator']>10]['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_twitter_archive['rating_numerator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rating_numerator greater than 14 drops off significantly\n",
    "sum(df_twitter_archive['rating_numerator']>14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_twitter_archive[df_twitter_archive['rating_numerator']>14][['tweet_id','text', 'rating_numerator']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_twitter_archive['name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess_2'></a>\n",
    "### *Assess 2. File from Internet: df_images*\n",
    "Image Prediction file that gives the top 3 predictions of a dog's breed based on a jpeg image associated with each tweet that has been run through a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Columns* \n",
    "- *tweet_id*: the last part of the tweet URL after \"status/\" and intersects with the *tweet_id* from `df_twitter_archive`\n",
    "- *jpg_url*: image url associated with tweet\n",
    "- *img_num*: image number that corresponds to the most confident prediction (1-4, sincetweets can have up to 4 images)\n",
    "- *p1*: the algorithm's #1 prediction for the image in the tweet\n",
    "- *p1_conf*: how confident the algorithm is in its #1 prediction\n",
    "- *p1_dog*: whether or not the #1 prediction is a breed of dog (TRUE/FALSE) \n",
    "- *p2*: the algorithm's second most likely prediction\n",
    "- *p2_conf*: how confident the algorithm is in its #2 prediction\n",
    "- *p2_dog*: whether or not the #2 prediction is a breed of dog\n",
    "- *p3*: the algorithm's third most likely prediction\n",
    "- *p3_conf*: how confident the algorithm is in its #3 prediction\n",
    "- *p3_dog*: whether or not the #3 prediction is a breed of dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_images.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_images['jpg_url'].duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Duplicated jpg_url likely due to retweets\n",
    "pd.merge(df_images[df_images['jpg_url'].duplicated(keep=False)][['tweet_id','jpg_url']], \n",
    "         df_twitter_archive[['tweet_id', 'retweeted_status_id']], \n",
    "         how='left', \n",
    "         on='tweet_id').sort_values(by='jpg_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_images['p1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_images['p2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_images['p3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_images['p1'], df_images['p2'], df_images['p3']]).sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.concat([df_images['p1'], df_images['p2'], df_images['p3']]).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_lower = pd.concat([df_images['p1'], df_images['p2'], df_images['p3']]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase =pd.concat([df_images['p1'], df_images['p2'], df_images['p3']]).str.lower().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(upper_lower - lowercase).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_lower['Cardigan'], upper_lower['cardigan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images[df_images['p2']==('cardigan')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess_3'></a>\n",
    "### *Assess 3. Twitter API data: df_tweets*\n",
    "Data gathered from Twitter's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Columns* \n",
    "- *tweet_id*: unique identifier for this Tweet.  Intersects with *tweet_id* in `df_twitter_archive`\n",
    "- *retweet_count*: number of times tweet has been retweeted\n",
    "- *favourite_count*: approximately how many times the tweet has been liked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_retweet_id=df_tweets.iloc[df_tweets['retweet_count'].idxmax()]['tweet_id']\n",
    "max_fav_id = df_tweets.iloc[df_tweets['favourite_count'].idxmax()]['tweet_id']\n",
    "max_retweet_id, max_fav_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_retweet_id=df_tweets.iloc[df_tweets['retweet_count'].idxmin()]['tweet_id']\n",
    "min_fav_id = df_tweets.iloc[df_tweets['favourite_count'].idxmin()]['tweet_id']\n",
    "min_retweet_id, min_fav_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive[df_twitter_archive['tweet_id']==max_retweet_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive[df_twitter_archive['tweet_id']==min_retweet_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive[df_twitter_archive['tweet_id']==min_fav_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tweets[df_tweets['favourite_count']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Discrepancy between 'retweet_count' and'favourite_count'=0 due to tweet_id relates to a retweet\n",
    "pd.merge(df_tweets[df_tweets['favourite_count']==0], df_twitter_archive[['tweet_id', 'retweeted_status_id']], how='left', on='tweet_id' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess_qual'></a>\n",
    "### *Assess 4. Quality Issues*\n",
    "* [Q1.](#qual_1) difference in number of records between the 3 tables: `df_twitter_archive`: 2356, `df_images`: 2075, `df_tweets`: 2337 \n",
    "\n",
    "`df_twitter_archive`\n",
    "* [Q2.](#qual_1) 181 *retweeted_status*, 78 *in_reply_to_status* - we only want original tweets\n",
    "    * [Q2.b.](#qual_2b) redundent columns: *in_reply_to_status_id*, *in_reply_to_user_id*, *retweeted_status_id*, *retweeted_status_user_id*, *retweeted_status_timestamp* \n",
    "* [Q3.](#qual_3) Missing data for *expanded_urls*, invalid entries with some entries having multiple urls\n",
    "* [Q4.](#qual_4) *source* column is difficult to read and contains extra/irrelevent information\n",
    "* [Q5.](#qual_5) Erroneous datatypes (*tweet_id* integer ==> string, *timestamp* string ==> datetime)\n",
    "* [Q6.](#qual_6) *rating_denominator* > 10\n",
    "    * [Q6.b.](#qual_6b)\n",
    "* [Q7.](#qual_6) Extra large *rating_numerator* (greater than 14)\n",
    "    * [Q7.b.](#qual_6b)\n",
    "* [Q8.](#qual_8) Missing *name* and 'a', 'an' and 'the' as names\n",
    "    * [Q8.b.](#qual_8b) Cooper, Meeko, BOOMER, Atlas, Loki,  Frank, Jack, Oliver\n",
    "\n",
    "`df_images`\n",
    "* [Q9.](#qual_5) *tweet_id* should be string\n",
    "* [Q10.](#tidy_4) Meaningless column headers : *p1*, *p1_conf*, *p1_dog*, *img_num* etc\n",
    "* [Q11.](#qual_11) 'Cardigan' and 'cardigan' (upper and lower case 'C')\n",
    "\n",
    "\n",
    "\n",
    "<a id='assess_tidy'></a>\n",
    "### *Assess 5. Tidiness Issues*\n",
    "* [T1.](#tidy_1)`df_twitter_archive` table:Stage of dog (*doggo*, *floofer*, *pupper*, *puppo*) one variable so should be one column\n",
    "    * [T1.b.](#tidy_1b) 10 cases of single tweets with multiple dog stages\n",
    "* [T2.](#tidy_2) *retweet_count* and *favourite_count* in `df_tweets` table should be part of `df_twitter_archive` table \n",
    "* [T3.](#tidy_3)`df_twitter_archive` table: multiple variable in *text* column.  Along with text, it contains a hyperlink, which has the same information as *expanded_url*\n",
    "* [T4.](#tidy_4) `df_images` table: column headers *p1*,*p2* and *p3* are values,not variable names.  The associated *_conf* and *_dog* columns into 2 columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='clean'></a>\n",
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of dataframes for cleaning\n",
    "df_twitter_archive_clean = df_twitter_archive.copy()\n",
    "df_images_clean = df_images.copy()\n",
    "df_tweets_clean = df_tweets.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean_miss'></a>\n",
    "### *Missing and irrelevant data*\n",
    "\n",
    "<a id='qual_1'></a>\n",
    "#### `treatments Q1 and Q2:`Remove: retweets, reply tos, tweets without image data in `df_images` and tweets that have since been deleted from server\n",
    "\n",
    "We only want original ratings (no retweets) that have images.\n",
    "1. Use the list of *tweet_id* from `df_images` as this has the least number of records (2075 vs 2356 in `df_twitter_archive` and 2337 in `df_tweets`) as base number of records\n",
    "2. Exclude the tweets that are retweets or replies.  Retweets and replies have a non-null value in *retweeted_status_id* and *in_reply_to_status_id* in `df_twitter_archive`\n",
    "3. Additionally compare with the *tweet_id* in `df_tweets` as this has less records than `df_twitter_archive` due to deleted records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create master_df that only has the data from df_twitter_archive which also has an image for that tweet_id\n",
    "master_df=pd.merge(df_images_clean['tweet_id'], df_twitter_archive_clean, how='left', on='tweet_id')\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete records that are retweets\n",
    "master_df = master_df[master_df['retweeted_status_id'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete records that are replies \n",
    "master_df = master_df[master_df['in_reply_to_status_id'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any records in master_df that don't have a corresponding tweet_id in df_tweets\n",
    "master_df = pd.merge(df_tweets_clean['tweet_id'],master_df, how='inner', on='tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update df_images to exclude any retweets, replies and deleted records\n",
    "df_images_clean = pd.merge(master_df['tweet_id'], df_images_clean, how='inner', on='tweet_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be empty\n",
    "print(master_df['retweeted_status_id'].value_counts())\n",
    "print(master_df['in_reply_to_status_id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records are more than other dataframes but this will be remedied in Tidy section\n",
    "df_tweets_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There shouldn't be any duplicated 'jpg_url' after retweets and replies are excluded\n",
    "sum(df_images_clean['jpg_url'].duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qual_2b'></a>\n",
    "#### `treatments Q2.b`: Redundant columns : *in_reply_to_status_id*, *in_reply_to_user_id*, *retweeted_status_id*, *retweeted_status_user_id*, *retweeted_status_timestamp*\n",
    "\n",
    "##### Define\n",
    "Drop the redundant columns: *in_reply_to_status_id*, *in_reply_to_user_id*, *retweeted_status_id*, *retweeted_status_user_id*, *retweeted_status_timestamp*  as they are all populated with NaN\n",
    "\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.drop(columns=['in_reply_to_status_id','in_reply_to_user_id',\n",
    "                       'retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'],\n",
    "              inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qual_3'></a>\n",
    "#### `treatments Q3`: Missing data for *expanded_urls*, invalid entries with some entries having multiple urls\n",
    "\n",
    "Records that are missing *expanded_urls* have already been excluded due to lack of image prediction in `df_images`\n",
    "\n",
    "Repopulate *extended_urls* due to some entries containing multiple urls, with some non-twitter urls.\n",
    "Read `json_tweet.txt` line by line to extract 'expanded_urls' and repopulate *expanded_urls* in `master_df` by creating `df_extended_url` and merging with `master_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_url = []\n",
    "url_error={}\n",
    "with open('{}/tweet_json.txt'.format(folder_name)) as f:\n",
    "    for line in f:\n",
    "        try: \n",
    "            json_data = json.loads(line)          \n",
    "            tweet_id = json_data['id']\n",
    "            if ('media') in json_data['entities'].keys():\n",
    "                url = json_data['entities']['media'][0]['expanded_url']\n",
    "            else:\n",
    "                url = json_data['entities']['urls'][0]['expanded_url']\n",
    "            \n",
    "            json_url.append({'tweet_id': tweet_id,\n",
    "                             'expanded_url': url})\n",
    "           \n",
    "        \n",
    "        except Exception as e:\n",
    "            url_error[json_data['id']]=str(e)\n",
    "\n",
    "print(len(url_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be 0 duplicates\n",
    "all_tweets = pd.Series(list(master_df['tweet_id']) + list(url_error.keys()))\n",
    "all_tweets[all_tweets.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The *tweet_ids* in url_error do not intersect with *tweet_id* in `master_df`.  Continue with repopulating *expanded_urls* in `master_df`.  If there are later any null entries, the *tweet_ids* in url_error will have to be re-visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded_urls = pd.DataFrame(json_url, columns=['tweet_id','expanded_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.merge(master_df, df_expanded_urls, how='left', on='tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop old 'expanded_urls'.  The new column 'expanded_url' also indicates that it is a singular url (drop s from urls)\n",
    "master_df.drop(columns='expanded_urls', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records missing 'expanded_url' - should be 0\n",
    "sum(master_df['expanded_url'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any 'expanded_url' that is not a twitter WeRateDogs url - should be 0\n",
    "master_df[master_df['expanded_url'].str[:30]!='https://twitter.com/dog_rates/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample check there is only one url in 'expanded_url'\n",
    "master_df.sample(20)[['tweet_id','expanded_url']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check there is only 1 'expanded_url' column\n",
    "master_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean_tidy'></a>\n",
    "### *Tidiness*\n",
    "<a id='tidy_1'></a>\n",
    "#### `treatments T1`: Stage of dog ('doggo' 'floofer' 'pupper' 'puppo') is one variable split over 4 columns\n",
    "\n",
    "##### Define\n",
    "\n",
    "Combine the *doggo*, *floofer*, *pupper* and *puppo* columns into one column named 'dog_stage'.  \n",
    "Delete redundant columns: *doggo*, *floofer*, *pupper* and *puppo*\n",
    "\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean=master_df.copy()\n",
    "master_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_clean['doggo'].value_counts())\n",
    "print(master_clean['floofer'].value_counts())\n",
    "print(master_clean['pupper'].value_counts())\n",
    "print(master_clean['puppo'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare 'doggo', 'floofer','pupper' and 'puppo' columns\n",
    "# Replace 'None' values with empty string ''\n",
    "master_clean['doggo'] = master_clean['doggo'].replace('None','', regex=True)\n",
    "master_clean['floofer'] = master_clean['floofer'].replace('None','', regex=True)\n",
    "master_clean['pupper'] = master_clean['pupper'].replace('None','', regex=True)\n",
    "master_clean['puppo'] = master_clean['puppo'].replace('None','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dog_stage column\n",
    "master_clean['dog_stage']= (master_clean['doggo'].str.strip()\n",
    "                            + master_clean['floofer'].str.strip() \n",
    "                            + master_clean['pupper'].str.strip() \n",
    "                            + master_clean['puppo'].str.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns:'doggo','floofer','pupper','puppo'\n",
    "master_clean.drop(columns=['doggo','floofer','pupper','puppo'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean['dog_stage'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tidy_1b'></a>\n",
    "#### `treatments T1.b`: 10 cases of single tweets with multiple dog stages\n",
    "\n",
    "\n",
    "##### Assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean[master_clean['dog_stage']=='doggopupper'][['tweet_id','text']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean[master_clean['dog_stage']=='doggofloofer'][['tweet_id','text']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "master_clean[master_clean['dog_stage']=='doggopuppo'][['tweet_id','text']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Individually make the following changes in `master_clean`\n",
    "1. If *dog_stage* is 'doggopuppo' \n",
    "        ==> change to 'puppo' (*tweet_id*:855851453814013952)\n",
    "2. If *dog_stage* is 'doggofloofer' \n",
    "        ==> change to 'doggo' (*tweet_id*:854010172552949760)\n",
    "3. If *dog_stage* is 'doggopupper' \n",
    "        ==> change to 'doggo' (*tweet_id*:785639753186217984)\n",
    "        ==> change to 'pupper' (*tweet_id*:817777686764523521,801115127852503040)\n",
    "        ==> change to 'multiple' (*tweet_id*:808106460588765185,759793422261743616,741067306818797568,733109485275860992)\n",
    "        ==> change to NaN (*tweet_id*:751583847268179968)\n",
    "        \n",
    "\n",
    "##### Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of tweet_id for dogs that need 'dog_stage' changed\n",
    "dog_list_change = (list(master_clean[master_clean['dog_stage']=='doggopuppo']['tweet_id']) + \n",
    " list(master_clean[master_clean['dog_stage']=='doggofloofer']['tweet_id']) + \n",
    " list(master_clean[master_clean['dog_stage']=='doggopupper']['tweet_id']))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'doggopuppo' to 'puppo'.  \n",
    "# Change 'doggofloofer' to 'doggo'\n",
    "master_clean['dog_stage'].replace('doggopuppo','puppo', inplace=True)\n",
    "master_clean['dog_stage'].replace('doggofloofer','doggo', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pupper_list=[817777686764523521,801115127852503040]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_list=[808106460588765185,759793422261743616,741067306818797568,733109485275860992]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 'doggopupper' to 'doggo' for tweet_id 785639753186217984\n",
    "master_clean.loc[master_clean['tweet_id']==785639753186217984,'dog_stage']='doggo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 'doggopupper' to 'pupper' for tweet_id in pupper_list\n",
    "for pupper in pupper_list:\n",
    "    master_clean.loc[master_clean['tweet_id']==pupper,'dog_stage'] = 'pupper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 'doggopupper' to 'multiple' for tweet_id in multiple_list\n",
    "for id_multi in multiple_list:\n",
    "    master_clean.loc[master_clean['tweet_id']==id_multi,'dog_stage'] = 'multiple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 'doggopupper' to '' for tweet_id 751583847268179968\n",
    "master_clean.loc[master_clean['tweet_id']==751583847268179968,'dog_stage']=np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean['dog_stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean[master_clean['tweet_id'].isin(dog_list_change)][['tweet_id','dog_stage']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tidy_2'></a>\n",
    "#### `treatments T2`: 'retweet_count' and 'favourite_count' should be part of main table, master\n",
    "\n",
    "##### Define\n",
    "\n",
    "Merge the *retweet_count* and *favourite_count* columns to the `master_clean` table, joining on *tweet_id*\n",
    "\n",
    "\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean = pd.merge(master_clean, df_tweets_clean, how='left', on='tweet_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tidy_3'></a>\n",
    "#### `treatments T3`: *text* column contains hyperlink\n",
    "\n",
    "##### Define\n",
    "\n",
    "Delete the hyperlink in the text column as they point to the information in *expanded_url*\n",
    "\n",
    "Right split *text* on 'https' and keep only the section to the left\n",
    "\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean['text'] = master_clean['text'].str.rsplit('https').str[0].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tidy_4'></a>\n",
    "#### `treatments T4`: column headers *p1*,*p2* and *p3* in `df_images` are values,not variable names.  The associated *_conf* and *_dog* columns into 2 columns\n",
    "\n",
    "#### `treatments Q10`: Meaningless column headers : p1, p1_conf, p1_dog, img_num etc\n",
    "\n",
    "\n",
    "##### Define\n",
    "Using a for loop, create dictionary of DataFrames `dict_predictions`, with p1, p2 and p3 as the dictionary keys.  \n",
    "\n",
    "Each dataframe would have the columns *tweet_id*, *prediction_image*, *image_number*, *prediction_number*, *prediction*, *confidence*, *is_dog*.  \n",
    "\n",
    "*prediction_number* is populated with (1-3), depending on p-number.\n",
    "\n",
    "Concatenate the 3 dataframes to create `df_image_prediction`\n",
    "\n",
    "\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_predictions={}\n",
    "\n",
    "for num in range(1,4):\n",
    "    p_level = 'p'+str(num)\n",
    "    dict_predictions[num] = df_images_clean[['tweet_id', \n",
    "                                  'jpg_url', \n",
    "                                  'img_num',\n",
    "                                  p_level,\n",
    "                                  p_level + '_conf',\n",
    "                                  p_level +'_dog']].rename(columns={p_level:'prediction',\n",
    "                                                                   (p_level + '_conf'): 'confidence',\n",
    "                                                                   (p_level +'_dog'): 'is_dog'})\n",
    "    dict_predictions[num]['prediction_number']=num\n",
    "\n",
    "    \n",
    "df_images_predictions=pd.concat(dict_predictions.values())\n",
    "#df_images_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Number of tweet_id in df_images_predictions divided by 3) - (number of tweet_id in df_images-_clean)\n",
    "# Divided by 3 as there are 3 p-levels\n",
    "# Should equal 0\n",
    "len(df_images_predictions['tweet_id'])/3 - len(df_images_clean['tweet_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images_predictions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_number should be 1-3\n",
    "df_images_predictions.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean_qual'></a>\n",
    "### *Quality*\n",
    "<a id='qual_4'></a>\n",
    "#### `treatments Q4`: *source* column is difficult to read and has unneccessary information\n",
    "\n",
    "##### Define\n",
    "Strip the unnecessary information in the source column to make the data readable and useful, by extracting the string between '>' and '</a'\n",
    "\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean['source']= master_clean['source'].str.extract(r'(.+)(>)(.+)(</a>)').iloc[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qual_5'></a>\n",
    "#### `treatments Q5 and Q9`: erroneous data types *tweet_id* and *timestamp*\n",
    "\n",
    "##### Define\n",
    "\n",
    "Convert *tweet_id* data type to string in `master_df` and `df_images_predictions`.  *tweet_id* is a unique identifier and will not be manipulated with maths. \n",
    "\n",
    "Convert *timestamp* data type to datetime\n",
    "\n",
    "\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean['tweet_id'] = master_clean['tweet_id'].astype(str).str.strip()\n",
    "master_clean['timestamp'] = pd.to_datetime(master_clean['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images_predictions['tweet_id'] = df_images_predictions['tweet_id'].astype(str).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images_predictions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qual_6'></a>\n",
    "#### `treatments Q6 and Q7`: *ratings_denominator* > 10 and extra large *ratings_numerator*\n",
    "\n",
    "\n",
    "#### Define\n",
    "\n",
    "After excluding retweets and replies, there are two reasons for *ratings_denominator* > 10 and extra large *ratings_numerator*:  \n",
    "    a) Selecting the incorrect chunk after parsing (including not recognizing decimals in the numerator)  \n",
    "    b) Multiple dogs\n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "1. Right Split *text* on '/' and <br>\n",
    " \n",
    " - select the last number (as a float, to account for decimals) before the '/' for *rating_numerator*  \n",
    " - select first number (as integer) after the '/' for the *rating_denominator*\n",
    "<br>    \n",
    "\n",
    "2. Recalibrate *rating_numerator* to it's ratio to 10 for any records with *rating_denominator* > 10, to account for multiple dogs.  \n",
    "\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tally of rating_numerator before cleaning for comparison\n",
    "master_clean['rating_numerator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repopulate 'rating_numerator' and 'rating_denominator' based on 'text'\n",
    "master_clean['rating_numerator']=master_clean['text'].str.rsplit('/',1).str[0].str.extract(r'(\\d+\\.?\\d*)$').astype(float)\n",
    "master_clean['rating_denominator'] = master_clean['text'].str.rsplit('/',1).str[-1].str.extract(r'^(\\d+)').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalibrate 'rating_numerator' to its ratio to 10 for records with 'rating_denominator' greater than 10\n",
    "master_clean.loc[master_clean['rating_denominator']>10, 'rating_numerator']= (master_clean.loc[master_clean['rating_denominator']>10, 'rating_numerator']\n",
    "                                                                      /master_clean.loc[master_clean['rating_denominator']>10, 'rating_denominator']\n",
    "                                                                      *10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the 'rating_numerator' with the before snapshot\n",
    "master_clean['rating_numerator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check details of records with rating_numerator >14\n",
    "master_clean[master_clean['rating_numerator']>14][['tweet_id','text','rating_numerator','rating_denominator']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  The records with rating_numerator > 14 will be excluded due to:\n",
    "1. No rating - 24/7 is not a rating (tweet_id:810984652412424192)\n",
    "2. Outlier - 1776/10 is a spoof rating (tweet_id:749981277374128128) \n",
    "3. Not a dog (tweet_id:670842764863651840)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rating_denominator for strange values\n",
    "master_clean['rating_denominator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check details of records with rating_denominator >10\n",
    "# rating_numerator should be the exaggerated rating_numerator divided by (rating_denominator times 10)\n",
    "master_clean[master_clean['rating_denominator']>14][['tweet_id','text','rating_numerator','rating_denominator']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qual_6b'></a>\n",
    "#### `treatments Q6.b and Q7.b`: Redundant column *rating_denominator*, exaggerated *rating_numerator*, column name *rating_numerator*\n",
    "\n",
    "##### Define\n",
    "\n",
    "Following from the treatment repopulating *rating_numerator* and *rating_denominator*\n",
    "1. Delete *rating_denominator* as all ratings are now out of 10\n",
    "2. Delete records with *rating_numerator* > 14 \n",
    "    a) No rating - 24/7 is not a rating (tweet_id:810984652412424192)\n",
    "    b) Outlier - 1776/10 is a spoof rating (tweet_id:749981277374128128) \n",
    "    c) Not a dog (tweet_id:670842764863651840)\n",
    "3. Rename *rating_numerator* to simply *rating* as there in no longer a denominator\n",
    "\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'rating_denominator' column\n",
    "master_clean.drop(columns='rating_denominator', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude records with 'rating_numerator' >14\n",
    "master_clean=master_clean[~(master_clean['rating_numerator']>14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'rating_numerator' to 'rating'\n",
    "master_clean.rename(columns={'rating_numerator':'rating'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be empty\n",
    "master_clean[master_clean['rating']>14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qual_8'></a>\n",
    "#### `treatments Q8`: Missing names and 'a', 'an' and 'the' as names\n",
    "\n",
    "##### Define\n",
    "1. Extract names from text using a regex expression finding names after \"This is\", \"Meet\", \"Say hello to\" or \"Here is\"\n",
    "2. Delete any non-names (eg \"a\", \"an\", \"the\"\n",
    "3. For records with missing names, extract names from text using a regex expression finding names after \"Named\".  This second pass is to account for text that have both \"This is\" + non-name and \"Named\" + name\n",
    "\n",
    "\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract names from 'text' following \"This is\", \"Meet\", \"Say hello to\" or \"Here is\"\n",
    "master_clean['new_name']=master_clean['text'].str.extract(\n",
    "    r'.*?([Tt]his\\sis|[Mm]eet|(?:[Ss]ay\\s)[Hh]ello(?:\\sto)|[Hh]ere\\sis)\\s([a-zA-ZÀ-ÿ\\']*)[\\.,]?\\s?.*').iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm list of new_name can be cleaned of non-names by deleting any names starting with a lowercase\n",
    "master_clean[master_clean['new_name'].str.islower() & ~(master_clean['new_name'].isnull())]['new_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all lowercase 'new_name' with NaN\n",
    "master_clean.loc[master_clean['new_name'].str.islower() & ~(master_clean['new_name'].isnull()),'new_name']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all 'new_name'=NaN, Extract name from text, when following \"Named\"\n",
    "master_clean.loc[master_clean['new_name'].isna(),'new_name'] = master_clean['text'].str.extract(\n",
    "    r'.*?([Nn]amed?)\\s([a-zA-ZÀ-ÿ\\']*)[\\.,]?\\s?.*').iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm list of new_name can be cleaned of non-names by deleting any names starting with a lowercase\n",
    "master_clean[master_clean['new_name'].str.islower() & ~(master_clean['new_name'].isnull())]['new_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all lowercase 'new_name' with NaN\n",
    "master_clean.loc[master_clean['new_name'].str.islower() & ~(master_clean['new_name'].isnull()),'new_name']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare 'name' and 'new_name' list for any glaring ommisions\n",
    "master_clean['new_name'].value_counts() - master_clean['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'name' column \n",
    "master_clean.drop(columns='name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'new_name' to 'name'\n",
    "master_clean.rename(columns={'new_name':'name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For non NaN 'name', the sum of lowercase 'name' should zero\n",
    "sum(master_clean[~master_clean['name'].isna()]['name'].str.islower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qual_8b'></a>\n",
    "#### `treatments Q8.b` Cooper, Meeko, BOOMER, Atlas, Loki,  Frank, Jack, Oliver\n",
    "\n",
    "##### Define\n",
    "Visual assessment noted 9 names that weren't picked up with regex expression\n",
    "Select rows where *text* contains 'Cooper', 'Meeko', 'BOOMER', 'Atlas', 'Loki', 'Frank', 'Jack', 'Oliver' and name is NaN and populate *name* with Upper-lower case standard\n",
    "\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_names = ['Cooper', 'Meeko', 'BOOMER', 'Atlas', 'Loki', 'Frank', 'Jack', 'Oliver']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for missing_name in missing_names:\n",
    "    master_clean.loc[(master_clean['text'].str.contains(missing_name\n",
    "                                                           )) & (\n",
    "        master_clean['name'].isnull()), 'name'] = missing_name.capitalize()                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean[master_clean['text'].str.contains('Cooper')][['tweet_id','text','name']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean[master_clean['text'].str.contains('Meeko')][['tweet_id','text','name']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean[master_clean['text'].str.contains('BOOMER')][['tweet_id','text','name']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean[master_clean['text'].str.contains('Atlas')][['tweet_id','text','name']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean[master_clean['text'].str.contains('Loki')][['tweet_id','text','name']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean[master_clean['text'].str.contains('Frank')][['tweet_id','text','name']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean[master_clean['text'].str.contains('Jack')][['tweet_id','text','name']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean[master_clean['text'].str.contains('Oliver')][['tweet_id','text','name']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qual_11'></a>\n",
    "#### `treatments Q11.`: Cardigan' and 'cardigan' (upper and lower case 'C')\n",
    "\n",
    "##### Define\n",
    "Standardize dog breed predictions so that 'Cardigan' (dog) is 'Cardigan Welsh Corgi' and not mistaken for 'cardigan' (clothing) \n",
    "\n",
    "Get list of dog breeds from American Kennel Club Wikipedia page using wptools and store in `df_dog_breeds`.  \n",
    "\n",
    "The url is: https://en.wikipedia.org/wiki/American_Kennel_Club#Recognized_breeds\n",
    "\n",
    "Clean list\n",
    "\n",
    "Map *prediction* in `df_images_predictions` where dog prediction is a dog, ie *is_dog*==True\n",
    "\n",
    "\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the page data from Wikipedia 'American Kennel Club'\n",
    "page_dog_breeds = wptools.page('American_Kennel_Club#Recognized_breeds').get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store links, that includes the list of dog breeds, in dataframe df_dog_breeds\n",
    "df_dog_breeds = pd.DataFrame(page_dog_breeds.data['links']).rename(columns={0:'dog_breed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of keywords to help clean list of non-dog breed entries\n",
    "list_non_dog=['group','show','kennel','trial','test','title','list','championship','club','breed',\n",
    "              'United States','Citizen', 'Fifteen and Send Time','Flyball', 'Internationale','International',\n",
    "              'disorder','dysplasia', 'Incorporation', 'Puppy mill','Lure','Miscellaneous','Pedigree','agility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the list of keywords in order to clean df_dog_breeds\n",
    "for non_dog in list_non_dog:\n",
    "    df_dog_breeds = df_dog_breeds[~df_dog_breeds['dog_breed'].str.lower().str.contains(non_dog.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize 'dog_breed' column : replace spaces' ' and hyphens'-' with underscores '_'\n",
    "# replace '_(dog)' with empty string ''\n",
    "df_dog_breeds['dog_breed']=df_dog_breeds['dog_breed'].str.replace(\" \",'_').str.replace(\"-\",'_').str.replace('_\\(dog\\)','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map and standardize dog breed to 'dog_breed' in df_dog_breeds\n",
    "def dog_breed_check(check_dog):\n",
    "\n",
    "    # dictionary of dog breeds that aren't captured in the main function\n",
    "    dog_dict = {'Brittany_spaniel': 'Brittany_dog',\n",
    "            'Pekinese': 'Pekingese',\n",
    "            'Saint_Bernard': 'St._Bernard',\n",
    "            'Scotch_terrier' : 'Scottish Terrier',\n",
    "            'Walker_hound' :'Treeing_Walker_Coonhound',\n",
    "            'wire_haired_fox_terrier' : 'Wire_Fox_Terrier'}\n",
    "    \n",
    "    try:\n",
    "        return (df_dog_breeds[df_dog_breeds['dog_breed'].str.lower()\n",
    "                              .str.replace('_','').str.contains(\n",
    "                                  check_dog.lower().replace('_',''))].iloc[0]['dog_breed'])\n",
    "       \n",
    "        \n",
    "    except:\n",
    "        if check_dog in dog_dict.keys():\n",
    "            return dog_dict[check_dog]\n",
    "        else:\n",
    "            return check_dog # if dog breed is not in df_dog_breeds, do not change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize 'prediction' column.  Replace hyphens '-' with underscore '_'\n",
    "df_images_predictions['prediction']=df_images_predictions['prediction'].str.replace('-','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map 'prediction' column to 'dog_breed' in df_dog_breeds where prediction is a dog, ie 'is_dog'==True\n",
    "df_images_predictions.loc[df_images_predictions['is_dog']==True,'prediction']=  \\\n",
    "    df_images_predictions.loc[df_images_predictions['is_dog']==True,'prediction'].apply(lambda x: dog_breed_check(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_images_predictions[(df_images_predictions['is_dog']==True) & \\\n",
    "                      (df_images_predictions['prediction_number']==1)]['prediction'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='store'></a>\n",
    "## Save Cleaned Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean.to_csv('{}/twitter_archive_master.csv'.format(folder_name), index=False)\n",
    "df_images_predictions.to_csv('{}/image_predictions.csv'.format(folder_name), index=False)\n",
    "df_dog_breeds.to_csv('{}/dog_breeds.csv'.format(folder_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analyze'></a>\n",
    "## Analyze and Visualize\n",
    "\n",
    "**WeRateDogs** *has exploded in popularity in the Twittersphere.  Can we pinpoint certain aspects of and patterns in ts Dog Rating Tweets that can help explain why the account has such a huge following?*\n",
    "\n",
    "Questions that will guide the analysis are:\n",
    "- [A1](#analyze_1) What is the average dog rating?\n",
    "- [A2](#analyze_4) Does a higher dog rating correlate with more popularity of the tweet?\n",
    "- [A3](#analyze_3) How often and when are tweets posted.  Is there a correlation between the day posted and dog rating/popularity of a tweet?\n",
    "- [A4](#analyze_4) General analysis to address questions such as\n",
    "    - What are the most common dog names?\n",
    "    - Do certain names tend to get a higher rating?  Tend to be more popular?\n",
    "    - Do tweets with a dog name tend to be more popular than tweets without a dog name?\n",
    "    - How does a dog's stage relate to a tweet's popularity\n",
    "- [A5](#analyze_5) Are more humourous tweets more popular?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analyze_1'></a>\n",
    "#### Distribution of Dog Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title and axis format\n",
    "title_font = {'fontname':'Arial', 'size':'16', 'color':'black', 'weight':'normal',\n",
    "  'verticalalignment':'bottom'} # Bottom vertical alignment for more space\n",
    "axis_font = {'fontname':'Arial', 'size':'14'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "sns.distplot(master_clean['rating'], kde=False)\n",
    "plt.title('Distribution of Dog Ratings', fontdict=title_font)\n",
    "plt.ylabel('Number of Ratings', fontdict=axis_font)\n",
    "plt.xlabel('Rating', fontdict=axis_font)\n",
    "plt.annotate(('Lower Quartile: {}\\nMedian: {}\\nUpper Quartile: {}'.format(\n",
    "    master_clean['rating'].describe()['25%'],\n",
    "    master_clean['rating'].describe()['50%'],\n",
    "    master_clean['rating'].describe()['75%'])),\n",
    "    xy=(0.05, 0.50), xycoords='axes fraction', fontname='Arial', size='14',\n",
    "             bbox=dict(facecolor='none', edgecolor='black', boxstyle='square, pad=0.7'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "The Dog Ratings system in unique in that it generally hands out scores greater than 10/10.  Scores are heavily left skewed and tend to the median of 11, maxing out at 14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of Popularity\n",
    "We can proxy a tweet's popularity by how many retweets and likes (favourites) it has, with a higher number in both equating to more popular.  Patterns relating to the popularity of the dog rating tweets can help explain the success of *WeRateDogs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation between Retweets and Likes\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.scatterplot(x='retweet_count', y='favourite_count', data=master_clean)\n",
    "plt.title('Correlation between number of Retweets and number of Likes', fontdict=title_font)\n",
    "plt.xlabel('Number of Retweets', fontdict=axis_font)\n",
    "plt.ylabel('Number of Likes',fontdict=axis_font)\n",
    "plt.annotate('Correlation (p-value) = {}'.format(\n",
    "    round(master_clean['retweet_count'].corr(master_clean['favourite_count'], method='pearson'), ndigits=4)),\n",
    "    xy=(0.05, 0.90), xycoords='axes fraction', fontname='Arial', size='14',\n",
    "             bbox=dict(facecolor='none', edgecolor='black', boxstyle='square, pad=0.3'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "Number of retweets and number of likes are highly positively correlated with each other, with a correlation coefficient of 0.9296.  Thus any further analysis involving popularity will just use *Number of Retweets* as proxy.  \n",
    "\n",
    "Furthermore, this aligns with our investigation of what contributes to the popularity and high following of *WeRateDogs*. While likes and favourites reflect the quality of engagement with followers, Retweets does this plus further distributes a tweet, reaching out to potential followers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analyze_3'></a>\n",
    "#### Timing of Tweet Posts\n",
    "Does the timing of when a tweet is posted correlate with how popular the tweet becomes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'hour' 'day_of_week' 'month' columns in master_clean based on 'timestamp'\n",
    "master_clean['hour']= master_clean['timestamp'].apply(lambda time: time.hour)\n",
    "master_clean['day_of_week']= master_clean['timestamp'].apply(lambda time: time.dayofweek)\n",
    "master_clean['month']= master_clean['timestamp'].apply(lambda time: time.month)\n",
    "master_clean['year']= master_clean['timestamp'].apply(lambda time: time.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map day of week integer to actual string name\n",
    "dmap = {0:'Mon', 1:'Tue', 2:'Wed',3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'}\n",
    "master_clean['day_of_week']= master_clean['day_of_week'].map(dmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets broken down by day of the week  per month\n",
    "day_month_count = master_clean.groupby(by=['day_of_week','month']).count()['tweet_id'].unstack()\\\n",
    ".sort_index(axis=1, ascending=True).reindex(order_days, axis=0)\n",
    "day_month_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of retweets broken down by day of the week per month\n",
    "day_month_retweet = master_clean.groupby(by=['day_of_week','month']).sum()['retweet_count'].unstack()\\\n",
    ".sort_index(axis=1, ascending=True).reindex(order_days, axis=0)\n",
    "day_month_retweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Timing 1` How have number of tweets and retweets compared over time?\n",
    "\n",
    "A comparison of Tweet posting and Retweet activity, to discover whether the number of tweet posted correlate with the number of retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean.groupby(master_clean['timestamp'].dt.to_period('m'))['tweet_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean.groupby(by=(master_clean['timestamp'].dt.to_period('m')))['retweet_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "\n",
    "#plot number of tweets over time\n",
    "master_clean.groupby(by=(master_clean['timestamp'].dt.to_period('m')))['tweet_id'].count().plot(colormap='Set1', \n",
    "                                                                                             figsize=(8,5))\n",
    "#set second y-axis\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "# plot number of retweets over time\n",
    "master_clean.groupby(by=(master_clean['timestamp'].dt.to_period('m')))['retweet_count'].sum().plot(colormap='tab10',\n",
    "                                                                                              figsize=(8,5))\n",
    "# Set title and axis names\n",
    "plt.title('Number of Tweets (Left Axis) and Retweets (Right Axis) over Time', fontdict=title_font)\n",
    "ax.set_ylabel('Number of Tweets', fontdict=axis_font)\n",
    "ax2.set_ylabel('Number of Retweets',fontdict=axis_font, \n",
    "               rotation=270, va='bottom')\n",
    "ax.set_xlabel('Date',fontdict=axis_font)\n",
    "\n",
    "# Label graph\n",
    "fig.text(x=0.73, y=0.19, s='Tweets\\n(left axis)', fontdict=axis_font,\n",
    "         bbox=dict(facecolor='white', edgecolor='black', boxstyle='square, pad=0.3'))\n",
    "fig.text(x=0.73, y=0.5, s='Retweets\\n(right axis)', fontdict=axis_font,\n",
    "         bbox=dict(facecolor='white', edgecolor='black', boxstyle='square, pad=0.3'))\n",
    "\n",
    "plt.tight_layout();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Timing 2` What is the distribution over a year of when Tweets are posted and when Popular Tweets are posted\n",
    "\n",
    "Compare the pattern of the distribution over a the year and broken down by day of the week of\n",
    "- Tweet Postings\n",
    "- Popular Tweet postings.\n",
    "\n",
    "Tweet postings identifies any pattern in posting activity over the course of a year broken down by day of the week\n",
    "\n",
    "Popular Tweet Postings identifes any pattern of when popular tweets are posted, by aggreagating the number of retweets, broken down by month and day of the week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap distribution of how popular tweets are based on when they are posted, \n",
    "# taking into account pattern on tweet posting\n",
    "figure,axes = plt.subplots(nrows=2, ncols=1, figsize=(14,14))\n",
    "\n",
    "sns.heatmap(day_month_count, cmap='coolwarm',linecolor='white', linewidth=3, ax=axes[0])\n",
    "axes[0].set_title('Distribution of When Tweets are Posted Over a Year', fontdict=title_font)\n",
    "axes[0].set_ylabel('Day of the Week')\n",
    "axes[0].set_xlabel('Month')\n",
    "\n",
    "sns.heatmap(day_month_retweet, cmap='coolwarm', linecolor='white', linewidth=3, ax=axes[1])\n",
    "axes[1].set_title('Distribution of Popular Tweet Postings Over a Year', fontdict=title_font)\n",
    "axes[1].set_ylabel('Day of the Week')\n",
    "axes[1].set_xlabel('Month')\n",
    "\n",
    "plt.tight_layout;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "Although the number of Tweets have dropped over time, the Retweet activity has remained elevated after a dip, and now trending over 2000 retweets per month.  The drop at the end is due to the dataset ending with an incomplete month of data. \n",
    "\n",
    "Interestingly, retweets were quite elevated towards the inception of the account, even though one would assume less followers.  This suggests that either a few Tweets were very popular early on or followers that jumped on board later on re-deiscovered and retweeted these early Tweets.  I think combined with the increased Tweeting activty early on both of these factors were at play and that these early Tweets were the impetus to the increasing popularity of *WeRateDogs*  \n",
    "\n",
    "More tweets are posted in the months of November and December.  However this does not coincide with when the most popular tweets are posted.  While tweets tended to be more popular if they were posted in December, the most popular tweet and/or tweets are posted on a Saturday in June.  \n",
    "\n",
    "Since the heatmap is an aggregation of retweets for each month and day of week, the hotspot in June could be due to posting of extremely popular posts that were retweeted extensively, or due to increased Tweet posting activity with a larger number of Tweets retweeted at a moderate level.  However, in conjunction with the heatmap showing the distribution of Tweet posting activity, the former reason is more feasible, that a few extremely popular post were retweeted extensively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analyze_4'></a>\n",
    "#### Analysing the most popular tweets\n",
    "\n",
    "Ordering the data based on popularity (*retweet_count*), common themes can be found as to what makes a tweet popular and thus contributing to the popularity and following of *WeRateDogs* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 Most common dog names\n",
    "master_clean['name'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe merging element of master and image_predictions \n",
    "master_image_pred_1=pd.merge(master_clean[['tweet_id','timestamp','month','day_of_week',\n",
    "                                           'rating','retweet_count','name', 'dog_stage']],\n",
    "        df_images_predictions[df_images_predictions['prediction_number']==1][['tweet_id', \n",
    "                                                                              'prediction','confidence','is_dog']],\n",
    "        on='tweet_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_image_pred_1['retweet_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_image_pred_1['timestamp'].min(), master_image_pred_1['timestamp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_image_pred_1.sort_values(by='retweet_count', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean[['tweet_id','timestamp','text','retweet_count', 'expanded_url']]\\\n",
    "                    .sort_values(by='retweet_count',ascending=False).head(10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean[master_clean['name']=='George']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_image_pred_1.sort_values(by='retweet_count', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How are dog stages represented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pie chart\n",
    "labels_dog_stage= list(master_clean['dog_stage'].value_counts().index)[1:]\n",
    "sizes_dog_stage = list(master_clean['dog_stage'].value_counts().values)[1:]\n",
    "labels_has_stage= ('No Dog Stage','')\n",
    "sizes_has_stage=list(master_clean['dog_stage'].value_counts().values)[0],sum(sizes_dog_stage)\n",
    "#colours\n",
    "colours_dog_stage=['#4a63d4','#b40426','#92b4fe','#d6dce4', '#ef896c']\n",
    "colours_has_stage = ['#DCDCDC','#f7b295']\n",
    "\n",
    "# Explode slice 'doggo' and 'Has Dog Stage'\n",
    "explode_dog_stage = (0,0.2,0,0,0)\n",
    "explode_has_stage=(0,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "\n",
    "axes1 = fig.add_axes([0.1, 0.1, 0.3, 0.3]) \n",
    "axes2 = fig.add_axes([0.4, 0.3, 0.6,0.6])\n",
    "\n",
    "axes1.pie(x=sizes_has_stage, explode=explode_has_stage,\n",
    "          colors=colours_has_stage, labels=labels_has_stage, \n",
    "          startangle=70,shadow=True)\n",
    "axes1.set_title('Distribution of\\nNo Dog Stage vs. Has Dog Stage')\n",
    "\n",
    "axes2.pie(x=sizes_dog_stage, explode=explode_dog_stage,\n",
    "          colors=colours_dog_stage, labels=labels_dog_stage, \n",
    "          shadow=True)\n",
    "axes2.set_title('Representation of Dog Stages' , fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bbox_props = dict(boxstyle=\"rarrow,pad=0.3\", fc=\"white\", ec=\"black\", lw=2)\n",
    "t = fig.text(0.4, 0.38, \"Has Dog Stage\", ha=\"center\", va=\"center\", rotation=30,\n",
    "            size=18,\n",
    "            bbox=bbox_props)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions\n",
    "\n",
    "- 3 out of 5 of the most popular tweets, based on retweets, were posted in June, with the top 2 both posted on a Saturday in June, 3 weeks apart.  In contrast the least popular 10 tweets were all posted in November 2015.  The least popular tweets occurred at the beginning of the dataset, when the *WeRateDogs* Twitter account was just beginning so understandably would not have many followers to retweet.  \n",
    "\n",
    "- Of note, in the top 10 popular tweets are tweets from early on in the dataset, on 14th and 20th December 2015.  According to image prediction , these are considered 'humourous' as their image predictions resulted in bubble and swing.  Perhaps with the popularity of these tweets began the rapid rise in popularity and followers of *WeRateDogs*.  It is interesting that around the time of these early popular tweets in December 2015, *WeRateDogs* significantly decreased the number of dog rating Tweets, which started at around 300 per month, peaking at about 350 in December 2015 and dropping to around 50 per month from April 2016, a level maintained since.  \n",
    "\n",
    "- Top 12 most popular tweets account for the top quartile for number of retweets, with a range occuring in the years 2015-2017, thus length of time since posting may not neccessarily be such a strong factor contributing to the large number of retweets.\n",
    "\n",
    "- Half the dogs in the most popular 10 tweets have a name, with neither of the top 2 Tweets having mention of the dog name.  Futhermore, only 1 of the 9 names in the 20 top popular posts are found in the top 20 most common dog names.  That name is Buddy, which is common dog name number 20. \n",
    "\n",
    "- 3 of the dogs in the top 10 are classified at the *doggo* dog stage.  In fact the number 1 and 2 popular Tweets are about *doggos*.  This is interesting, considering that doggos represent a small percentage of the total tweets, and if we exclude tweets without a dog stage mentioned, *doggos* still only represent about a quarter.\n",
    "\n",
    "- The top 20 most popular Tweets have ratings above 10, with 16 of the Tweets having ratings in the upper quartile.  In contrast the 10 least popular Tweets have ratings 11 and below, with 6 having a rating in the lower quartile.  Again these least popular tweets were in the infancy of the account and most likely before the workings of the generous rating system were ironed out\n",
    "\n",
    "- The top 3 most popular Tweets included a video and according to the text, the dog is doing something that is not usual for a dog\n",
    "    - Here's a doggo realizing you can stand in a pool.\n",
    "    - Here's a doggo blowing bubbles.\n",
    "    - This is Stephan. He just wants to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analyze_5'></a>\n",
    "#### Is there a relationship between humourous tweets and dog rating or popularity?\n",
    "\n",
    "*WeRateDogs* is known for its humourous Tweets.  Perhaps humourous tweets are more popular and readily shared, thus increasing engagement and followers.\n",
    "\n",
    "`Humour 1`If we first define a humourous tweet as one where the dog may not be so easily visible in the photo due to perhaps blending in to the background or in costume, we could use the image prediction data.  A prediction that is not a dog could be a proxy for humour.  We will use the data from the most confident prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the relationship between ratings and retweet_count, \n",
    "#separating for whether the most confident image prediction was a dog or not\n",
    "\n",
    "#plt.figure(figsize=(12,12))\n",
    "\n",
    "g=sns.lmplot(x='rating',y='retweet_count',data=master_image_pred_1,col='is_dog', fit_reg=False,\n",
    "           scatter_kws={'s':100},palette='coolwarm', height=5, aspect=1, legend=False)\n",
    "plt.subplots_adjust(top=0.7)\n",
    "g.fig.suptitle('Popularity vs. Dog Rating: Comparing Distribution of Humorous Tweets\\nBased on Image Prediction', \n",
    "               fontdict=title_font, fontsize='16')\n",
    "g.set_xlabels('Dog Rating', fontdict=axis_font)\n",
    "g.set_ylabels('Popularity (Number of Retweets)', fontdict=axis_font)\n",
    "\n",
    "g.fig.axes[0].set_title('Humorous Tweet\\nImage Prediction: Not a Dog')\n",
    "g.fig.axes[1].set_title('Non-Humorous Tweet\\nImage Prediction: A Dog')\n",
    "\n",
    "plt.tight_layout;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Humour 2`  Alternatively, we could search the tweet text for common 'joke' phrases, suggesting there is no dog in the picture.  \n",
    "\n",
    "Common joke phrases are: \n",
    "- We only rate dogs\n",
    "- Please only send photos of dogs\n",
    "- Another photo without a dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows in master_clean that contain joke phrases in 'text' and set 'humourous_comment' to True\n",
    "master_clean.loc[master_clean['text'].str.lower().str.contains('we only rate dogs') | \\\n",
    "                 master_clean['text'].str.lower().str.contains('please only send') | \\\n",
    "                 master_clean['text'].str.lower().str.contains('without a dog'),'humourous_comment']=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'humourous_comment' set to False if text does not contain joke phrase\n",
    "master_clean.loc[master_clean['humourous_comment'].isnull(),'humourous_comment']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between ratings and retweet_count, \n",
    "#separating for whether the text contained a joke phrase\n",
    "g=sns.lmplot(x='rating',y='retweet_count',data=master_clean,hue='humourous_comment', fit_reg=False,\n",
    "           markers=['o','v'],scatter_kws={'s':200},\n",
    "             palette='coolwarm', height=5, aspect=2, legend=False, legend_out=True)\n",
    "plt.subplots_adjust(top=0.8)\n",
    "g.fig.suptitle('Popularity vs. Dog Rating: Comparing Distribution of Humorous Tweets\\n(Based on Text)', \n",
    "               fontdict=title_font, fontsize='16')\n",
    "g.set_xlabels('Dog Rating', fontdict=axis_font)\n",
    "g.set_ylabels('Popularity (Number of Retweets)', fontdict=axis_font)\n",
    "\n",
    "plt.legend(title='Humorous Tweet', loc='upper left')\n",
    "plt.tight_layout;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "Comparing Popularity of a Tweet with the Dog Rating given, there does appear that higher dog rating Tweets are more popular.  However we must take into consideration that over 75% of the Tweets have a dog rating greater than 10, thus increasing the possibility that tweets with higher ratings become more popular.\n",
    "\n",
    "When using either Image Prediction or Text as proxy for humour, the humourous Tweets are not the the most popular and otherwise trend similarly to non-humourous Tweets. \n",
    "\n",
    "This is however an imperfect system to define humourous Tweets.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## Conclusions\n",
    "- Dog Ratings, while generous and routinely above 10/10, still trend around the median of 11 and max out at 14 \n",
    "- Tweeting activity giving out Dog Ratings has decreased significantly since inception of the *WeRate ogs* account, while Retweets have remained elevated.  The early Tweets were retweeted extensively and possibly were the impetus for the huge following of the *WeRateDogs* account to the point where retweet activity is sustained even though Tweeting dog ratings has decresed.\n",
    "- Tweeting activity is generally higher in November and December with tweets posted in December and January more popular.  However tweets posted on Saturday in June were retweeted the most.  This can be attributed to the two most popular Tweets.\n",
    "- The top 12 Tweets, out of nearly 2000, account for the top quartile of popularity (number of retweets).  These tweets span across the years 2015-2017\n",
    "- 5 of the dogs in the 10 most Popular Tweets had their name mentioned.  Only one of the 9 names mentioned in the 20 most popular Tweeets were in the top 20 common names in the dataset\n",
    "- *doggo* dog stage was disproportionately represented in the top 10 Tweets, 3 out 10, with 2 of them the 2 most popular tweets.\n",
    "- Humourous Tweets aren't generally more popular than non-humours tweets.  This could be due to an imperfect system in defining humourous tweets among a set of Tweets that are known to be humourous.\n",
    "- The top 3 popular Tweets included a video showing activities not normally done by a dog - standing in a pool, blowing bubbles, just trying to help.\n",
    "\n",
    "\n",
    "### Extensions\n",
    "\n",
    "##### Data Wrangling\n",
    "Data wrangling of the *WeRateDog* Twitter data uncovered both *Quality* and *Tidiness* issues.  A bulk of the assessment and cleaning involved reading the Twitter text.  Thus being able somehow to assess programatically as opposed to visually may be more time efficient and less prone to human error.\n",
    "\n",
    "##### Analysis\n",
    "Other interesting avenues of analysis could include:\n",
    "* How have the Dog Ratings trended over time?\n",
    "* How has the number of followers trended over time?\n",
    "    - Were there spikes in number of followers that coincide a Tweet?\n",
    "* Are there certain key words in the text of the most popular Dog Rating Tweets?\n",
    "* The timing of retweets from retweet date as opposed to date of original post\n",
    "* Are Tweets with a video included more popular?   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources### \n",
    "\n",
    "[6 signs of \n",
    "dirty data](https://www.umbel.com/blog/audience-data/6-signs-dirty-data-infographic/)\n",
    "\n",
    "[check encoding of file](https://stackoverflow.com/questions/37177069/how-to-check-encoding-of-a-csv-file)\n",
    "\n",
    "https://stackoverflow.com/questions/28384588/twitter-api-get-tweets-with-specific-id\n",
    "\n",
    "https://knowledge.udacity.com/questions/9781\n",
    "\n",
    "[json.dump each variable on new line](https://stackoverflow.com/questions/17055117/python-json-dump-append-to-txt-with-each-variable-on-new-line)\n",
    "\n",
    "https://stackoverflow.com/questions/21308762/avoid-twitter-api-limitation-with-tweepy\n",
    "\n",
    "https://stackoverflow.com/questions/18869688/twitter-api-check-if-a-tweet-is-a-retweet\n",
    "\n",
    "https://knowledge.udacity.com/questions/33009\n",
    "\n",
    "https://stackoverflow.com/questions/49161120/pandas-python-set-value-of-one-column-based-on-value-in-another-column\n",
    "\n",
    "[sort weekday on heatmap](https://stackoverflow.com/questions/47741400/pandas-dataframe-group-and-sort-by-weekday?rq=1)\n",
    "\n",
    "[create dataframes in a loop](https://stackoverflow.com/questions/55277587/create-dataframe-in-a-loop)\n",
    "\n",
    "[Dog breed list](https://en.wikipedia.org/wiki/American_Kennel_Club#Recognized_breeds)\n",
    "\n",
    "[Implot Title](https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
